{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tkbharadwaj/vggnet/blob/main/VGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "https://github.com/tkbharadwaj/vggnet\n",
        "```\n",
        "Tejas Bharadwaj <br>\n",
        "7/6/23 <br>\n",
        "Classifier for CIFAR-10 Dataset <br>\n",
        "Goal: 90% testing accuracy <br>\n",
        "Some code borrowed & modified from:\n",
        "\n",
        "```\n",
        "https://github.com/patrickloeber/pytorchTutorial/blob/master/14_cnn.py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mDWVkEiuDXyO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4fuoKP-bdiT"
      },
      "source": [
        "Import Modules, Initialize Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3jAo4l09Y9CF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from tqdm import trange\n",
        "device = torch.device(\"cuda:0\")\n",
        "batch_size = 128\n",
        "learning_rate = .001\n",
        "num_epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsQuP3jhbmWn"
      },
      "source": [
        "Load Dataset,\n",
        "normalize + transform using RandomCrop and HorizontalFlip for model's resilience"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvi94ess632E"
      },
      "source": [
        "Dataset used is CIFAR-10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdWJS9A1LkNt"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [ transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))],)\n",
        "train_set = torchvision.datasets.CIFAR10(root = \"/content\", download = True, train = True, transform =transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transform]))\n",
        "test_set = torchvision.datasets.CIFAR10(root = \"/content\", download = True, train = False, transform = transform)\n",
        "\n",
        "train_load = torch.utils.data.DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
        "test_load = torch.utils.data.DataLoader(test_set, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "classes = ('airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship',  'truck')\n",
        "\n",
        "print(train_load)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkHTze92b0kR"
      },
      "source": [
        "Implementation of VGG-13 Network by Tejas Bharadwaj <br>\n",
        "Original Paper: ``` https://arxiv.org/pdf/1409.1556v6.pdf ``` <br>\n",
        "Weight Initialization done according to \"Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)\" <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi2ki61UXzP9"
      },
      "outputs": [],
      "source": [
        "class VGG(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3,64, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64,64, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64,128, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128,128, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "            )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(128,256, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256,256, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(256,512, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512,512, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(512,512, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512,512, kernel_size=3, stride =1, padding = 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                torch.nn.init.xavier_uniform_(m.weight.data, gain = 0.5)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "      #conv layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "      #fully connected layers\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = VGG()\n",
        "model = model.to(device)\n",
        "summary(model, (3, 32 , 32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GDkTz72J0VUN"
      },
      "outputs": [],
      "source": [
        "def learning_rate_decay(model, optimizer, decay = 0.5):\n",
        "  for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = decay*param_group['lr']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "vali_loss_history = []\n",
        "train_loss_history = []\n",
        "accuracy_history = []\n",
        "\n",
        "tl_count = 0\n",
        "vl_count = 0\n",
        "\n",
        "plt.ylim([0, 100])\n",
        "plt.xlim([0, 250])\n",
        "plt.xlabel(\"#epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "H5gtBelsdde1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKL8qHy3b78b"
      },
      "source": [
        "Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si_P249XQM2K"
      },
      "outputs": [],
      "source": [
        "plt.show()\n",
        "num_epochs = 30\n",
        "count = 1\n",
        "count2 = 0\n",
        "learning_rate = .01\n",
        "decay_after_epochs = [50, 20, 10]\n",
        "model = VGG().to(device)\n",
        "wd = 5*10**(-4) #from original paper\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = wd)\n",
        "\n",
        "n_total_steps = len(train_load)\n",
        "\n",
        "with trange(1, num_epochs+1) as t:\n",
        "  mloss = 0\n",
        "  for epoch in t:\n",
        "      x.append(epoch)\n",
        "      model.train()\n",
        "      #learning rate decay ** make into function\n",
        "      if count % decay_after_epochs[count2] == 0:\n",
        "        learning_rate_decay(model, optimizer, 1)\n",
        "        count = 0\n",
        "        count2 = count2 + 1\n",
        "      count += 1\n",
        "      train_loss_history.append(0)\n",
        "      for i, (images, labels) in enumerate(train_load):\n",
        "          images = images.to(device)\n",
        "          outputs = model(images)\n",
        "          outputs = outputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          loss = criterion(outputs, labels)\n",
        "          #print(loss)\n",
        "          train_loss_history[tl_count] += loss.item()\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          mloss = loss.item()\n",
        "      train_loss_history[tl_count] /= len(train_load)\n",
        "      tl_count += 1\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        n_correct = 0\n",
        "        n_samples = 0\n",
        "        n_class_correct = [0 for i in range(10)]\n",
        "        n_class_samples = [0 for i in range(10)]\n",
        "        vali_loss_history.append(0)\n",
        "        for images, labels in test_load:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            labels = labels.to(device)\n",
        "            outputs = outputs.to(device)\n",
        "            loss = criterion(outputs, labels)\n",
        "            vali_loss_history[vl_count] += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            n_samples += labels.size(0)\n",
        "            n_correct += (predicted == labels).sum().item()\n",
        "            for i in range(batch_size):\n",
        "                if i >= len(labels):\n",
        "                  break;\n",
        "                label = labels[i]\n",
        "                pred = predicted[i]\n",
        "                if (label == pred):\n",
        "                    n_class_correct[label] += 1\n",
        "                n_class_samples[label] += 1\n",
        "        vali_loss_history[vl_count] /= len(test_load)\n",
        "        vl_count += 1\n",
        "\n",
        "\n",
        "        acc = 100.0 * n_correct / n_samples\n",
        "        t.set_description('Epoch %i' % epoch)\n",
        "        t.set_postfix(training_loss=mloss, test_accuracy=acc)\n",
        "        accuracy_history.append(acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depict the models' training and validation loss in order to optimize #epochs hyperparameter \\*\\*make dynamic plotting\\*\\*"
      ],
      "metadata": {
        "id": "Pqpbj1TbUxcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ylim([0, 2.5])\n",
        "plt.xlim([0, num_epochs+2])\n",
        "plt.xlabel(\"#epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "print(train_loss_history)\n",
        "plt.plot(x[0:80], train_loss_history, label = \"Training\")\n",
        "\n",
        "print(vali_loss_history)\n",
        "plt.plot(x[0:80], vali_loss_history, label = \"Validation\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ft5WTtd4QS3z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}